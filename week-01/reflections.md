# Week 1 Reflections

## What I Learned
- Transformers break input into tokens and apply attention.
- Dot product = focus mechanism.

## What Felt Hard
- Understanding how multi-head attention works.

## Questions I Have
- Why do we need keys and values separately?

## Ideas Sparked
- Can I visualize attention with real text from ChatGPT?
